\section{PAC-Bayes and Bayesian Inference}

Theorem~\ref{thm:catoni}, and in particular Corollary~\ref{cor:gibbs} provide
interesting parallels between the structure of an optimal posterior and the
posteriors from Bayes update rules. In this section, we explore this link in
more detail.

\subsection{The Case of Bounded Log Likelihood}

Let us assume that the hypothesis class $\CH$ is in fact a set of models mapping
features in $\CX$ to labels in $\CY$, and these models are parametrized by
$\theta \in \Theta$, i.e.\ $\CH = \{h_\theta : \theta \in \Theta\}$. Let us
impose a prior $\pi(\theta)$ on this parameter space---this is equivalent to
imposing a prior on the hypothesis class directly. The likelihood of observing
observing sample $S = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ is given by
\begin{equation}
  p(S \mid \theta) = \prod_{i = 1}^n p(x_i, y_i \mid \theta)
\end{equation}
Now, let us use the negative log-likelihood as our loss function:
\begin{equation}
  L_\text{nll}(h_\theta, x, y) = - \log p(x, y \mid \theta)
\end{equation}
Then the empirical risk is given by
\begin{equation}
  \hat{R}_S^{L_\text{nll}}(\theta) := \hat{R}_S^{L_\text{nll}}(h_\theta) =
  \frac{1}{n} \sum_{i = 1}^n L_\text{nll}(h_\theta, x_i, y_i) = -\frac{1}{n}
  \sum_{i = 1}^n \log p(x_i, y_i \mid \theta) = - \frac{1}{n} \log p(S \mid
  \theta)
\end{equation}
or conversely, the likelihood is given by
\begin{equation}
  p(S \mid \theta) = \exp(-n \hat{R}_S^{L_\text{nll}}(\theta))
\end{equation}
We can now compute the posterior using Bayes rule:
\begin{equation}
  p(\theta \mid S) = \frac{\pi(\theta) p(S \mid \theta)}{p(S)} =
  \frac{\pi(\theta) \exp(-n \hat{R}_S^{L_\text{nll}}(\theta))}{p(S)}
\end{equation}
This corresponds exactly to the optimal Gibbs posterior $\rho^*$, provided that
the negative log-likelihood loss is bounded:
\begin{equation}
  \pi(h_\theta) = \pi(\theta), \quad Z_S = p(S), \quad \rho^*(h_\theta) =
  p(\theta \mid S)
\end{equation}
Additionally, we obtain
\begin{equation}
  \begin{split}
    n \Ev[\theta \sim \rho^*]{\hat{R}_S^{L_\text{nll}}(\theta)} +
    \KL{\rho^*}{\pi}
    &= \int_\Theta \rho^*(\theta) \left[n \hat{R}_S^{L_\text{nll}}(\theta) + \ln
    \frac{\rho^*(\theta)}{\pi(\theta)}\right] d\theta \\
    &= \int_\Theta \rho^*(\theta) \left[n \hat{R}_S^{L_\text{nll}}(\theta) + \ln
    \frac{\frac{1}{Z_S} \pi(\theta) \exp(-n
    \hat{R}_S^{L_\text{nll}}(\theta))}{\pi(\theta)}\right] d\theta \\
    &= \int_\Theta \rho^*(\theta) \left[\ln \frac{1}{Z_S}\right] d\theta \\
    &= -\ln Z_S
  \end{split}
\end{equation}
Clearly, minimizing the PAC-Bayes bound is equivalent to maximizing the evidence
$Z_S = p(S)$ of the sample. This allows us to reformulate
Theorem~\ref{thm:catoni} in as follows:

\begin{corollary}
  \label{cor:catoni}
  Given a data distribution $\CD$ over $\CX \times \CY$, a parameter set
  $\Theta$, a prior $\pi(\theta)$ over this parameter set and a real number
  $\delta \in (0, 1)$, if the negative log likelihood $L_\text{nll}(h_\theta, x,
  y) = -\ln p(x, y \mid \theta)$ lies in $[a, b]$, then we have
  \begin{equation}
    \Ev[\theta \sim \rho^*]{R_\CD^{L_\text{nll}}(\theta)} \leq a + \frac{b -
    a}{1 - e^{a - b}}\left[1 - e^{2 a - b} \sqrt[n]{p(S) \delta}\right]
  \end{equation}
  with probability at least $1 - \delta$ over i.i.d.\ sample $S \sim \CD^n$.
\end{corollary}

\subsection{Handling Unbounded Log Likelihoods}

We can artificially ensure that the negative log-likelihood is bounded, with
range $[a, b]$, by assigning zero prior probability to those parameter values
that violate this condition. Naturally, this ``restrict support'' approach is
severely limiting. We therefore aim to develop analogs of
Theorem~\ref{thm:catoni} for unbounded loss functions.

In \cite{alquier2016properties}, the authors present a general PAC-Bayes bound
that does not make any assumptions on the boundedness of the loss function:

\begin{theorem}[Alquier et al.\ PAC-Bayes Bound]
  Given a distribution $\CD$ over $\CX \times \CY$, a hypothesis set $\CH$, a
  loss function $L : \CH \times \CX \times \CY \to \RR$, a prior distribution
  $\pi$ over $\CH$ and real numbers $\delta \in (0, 1)$ and $\lambda > 0$, the
  following holds with probability at least $1 - \delta$ over $S \sim \CD^n$
  \begin{equation}
    \label{eq:alquier-bound}
    \Ev[h \sim \rho]{R_\CD^L(h)} \leq \Ev[h \sim \rho]{\hat{R}_S^L(h)} +
    \frac{1}{\lambda}\left[\KL{\rho}{\pi} + \ln \frac{1}{\delta} +
    \psi_\lambda^L(\pi, \CD, n)\right]
  \end{equation}
  with
  \begin{equation}
    \psi_\lambda^L(\pi, \CD, n) = \ln\left(\Ev[h \sim \pi]{\Ev[S' \sim
    \CD^n]{\exp(\lambda(R_\CD^L(h) - \hat{R}_S^L(h)))}}\right)
  \end{equation}
  for all distributions $\rho$ on $\CH$.
\end{theorem}

\begin{proof}
  This statement can be recovered from Theorem~\ref{thm:pac-bayes} by using
  \begin{equation}
    \Delta_\lambda(p, q) = \frac{\lambda}{n} (p - q)
  \end{equation}
  as a convex function with $I = \RR$.
\end{proof}

As an example, let us derive a bound for $\psi_\lambda^L(\pi, \CD, n)$ when the
loss function is bounded in $[a, b]$. We will need Hoeffding's lemma:

\begin{lemma}[Hoeffding's Lemma]
  Let $X$ be a random variable with $c \leq X \leq d$ a.s.\ and $\Ev{X} = \mu$.
  Then
  \begin{equation}
    \Ev{e^{\lambda X}} \leq \exp(\lambda \eta + \frac{\lambda^2 (d - c)^2}{8})
  \end{equation}
\end{lemma}

\begin{corollary}[Alquier et al.\ PAC-Bayes Bound for Bounded Loss]
  Given a distribution $\CD$ over $\CX \times \CY$, a hypothesis set $\CH$, a
  loss function $L : \CH \times \CX \times \CY \to [a, b]$, a prior distribution
  $\pi$ over $\CH$ and real numbers $\delta \in (0, 1)$ and $\lambda > 0$, the
  following holds with probability at least $1 - \delta$ over $S \sim \CD^n$
  \begin{equation}
    \label{eq:alquier-bound-bounded-loss}
    \Ev[h \sim \rho]{R_\CD^L(h)} \leq \Ev[h \sim \rho]{\hat{R}_S^L(h)} +
    \frac{1}{n}\left[\KL{\rho}{\pi} + \ln \frac{1}{\delta}\right] + \frac{(b -
    a)^2}{2}
  \end{equation}
\end{corollary}

\begin{proof}
  We compute
  \begin{equation}
    \begin{split}
      \Ev[S' \sim \CD^n]{\exp(\lambda (R_D^L(h) - \hat{R}_S^L(h)))}
      &= \Ev[S' \sim \CD^n]{\exp(-\frac{\lambda}{n} \sum_{i = 1}^n [L(h, x_i',
      y_i') - R_\CD^L(h)])} \\
      &= \left[\Ev[(x', y') \sim \CD]{\exp(-\frac{\lambda}{n} [L(h, x', y') -
      R_\CD^L(h)])}\right]^n
    \end{split}
  \end{equation}
  where the second step follow from i.i.d.\ sampling of $S$ from $\CD^n$. Now,
  $L(h, x', y') - R_\CD^L(h)$ is a random variable of $(x', y') \sim \CD$ with
  mean $0$ and range $[a - b, b - a]$. Hence from Hoeffding's lemma, we have
  \begin{equation}
    \Ev[(x', y') \sim \CD]{\exp(-\frac{\lambda}{n} [L(h, x', y') - R_\CD^L(h)])}
    \leq \exp(\frac{\lambda^2}{n^2} \frac{[2 (b - a)]^2}{8})
  \end{equation}
  and the right hand side is at least $1$. It follows that
  \begin{equation}
    \Ev[S' \sim \CD^n]{\exp(\lambda (R_D^L(h) - \hat{R}_S^L(h)))} \leq
    \exp(\frac{\lambda^2 (b - a)^2}{2n}) \implies \psi_\lambda^L(\pi, \CD, n)
    \leq \frac{\lambda^2 (b - a)^2}{2 n}
  \end{equation}
  Substituting $\lambda = n$, we get our desired result.
\end{proof}

We note that with the choice of $\lambda = \sqrt{n}$, we get the following
alternate to \eqref{eq:alquier-bound-bounded-loss}
\begin{equation}
  \Ev[h \sim \rho]{R_\CD^L(h)} \leq \Ev[h \sim \rho]{\hat{R}_S^L(h)} +
  \frac{1}{\sqrt{n}}\left[\KL{\rho}{\pi} + \ln \frac{1}{\delta} + \frac{(b -
  a)^2}{2}\right]
\end{equation}
This is certainly a better bound from the generalization perspective---as $n \to
\infty$, the bound converges to the training loss given fixed $\rho$. But this
loses the Bayes rule interpretation of the optimal posterior.

We now introduce two classes of loss functions for which we can easily bound the
$\psi_\lambda^L$ term:

\begin{definition}[Sub-Gaussian Loss]
  A loss function $L$ is said to be sub-Gaussian with variance factor $s^2$
  w.r.t.\ prior $\pi$ and data distribution $\CD$ if
  \begin{equation}
    \ln \Ev[h \sim \pi, (x, y) \sim \CD]{\exp(\lambda [R_\CD^L(h) - L(h, x,
    y)])} \leq \frac{\lambda^2 s^2}{2}
  \end{equation}
  for all $\lambda \in \RR$.
\end{definition}

\begin{definition}[Sub-Gamma Loss]
  A loss function $L$ is said to be sub-gamma with variance factor $s^2$ and
  scale factor $c > 0$ w.r.t.\ prior $\pi$ and data distribution $\CD$ if
  \begin{equation}
    \ln \Ev[h \sim \pi, (x, y) \sim \CD]{\exp(\lambda [R_\CD^L(h) - L(h, x,
    y)])} \leq \frac{s^2}{c^2} [-\ln(1 - \lambda c) - \lambda c] \leq
    \frac{\lambda^2 c^2}{2 (1 - c \lambda)}
  \end{equation}
  for all $\lambda \in (0, 1 / c)$.
\end{definition}

These definitions are tailor-made for the following two immediate corollaries:

\begin{corollary}[Alquier et al.\ PAC-Bayes Bound for Sub-Gaussian Loss]
  Given a distribution $\CD$ over $\CX \times \CY$, a hypothesis set $\CH$, a
  prior distribution $\pi$ over $\CH$, a loss function $L : \CH \times \CX \times
  \CY \to \RR$ sub-Gaussian w.r.t.\ $\pi$ and $\CD$, and real numbers $\delta \in
  (0, 1)$ and $\lambda > 0$, the following holds with probability at least $1 -
  \delta$ over $S \sim \CD^n$
  \begin{equation}
    \label{eq:alquier-bound-sub-gaussian-loss}
    \Ev[h \sim \rho]{R_\CD^L(h)} \leq \Ev[h \sim \rho]{\hat{R}_S^L(h)} +
    \frac{1}{n}\left[\KL{\rho}{\pi} + \ln \frac{1}{\delta}\right] +
    \frac{s^2}{2}
  \end{equation}
\end{corollary}

\begin{corollary}[Alquier et al.\ PAC-Bayes Bound for Sub-Gamma Loss]
  Given a distribution $\CD$ over $\CX \times \CY$, a hypothesis set $\CH$, a
  prior distribution $\pi$ over $\CH$, a loss function $L : \CH \times \CX \times
  \CY \to \RR$ sub-gamma w.r.t.\ $\pi$ and $\CD$, and real numbers $\delta \in
  (0, 1)$ and $\lambda > 0$, the following holds with probability at least $1 -
  \delta$ over $S \sim \CD^n$
  \begin{equation}
    \label{eq:alquier-bound-sub-gamma-loss}
    \Ev[h \sim \rho]{R_\CD^L(h)} \leq \Ev[h \sim \rho]{\hat{R}_S^L(h)} +
    \frac{1}{n}\left[\KL{\rho}{\pi} + \ln \frac{1}{\delta}\right] +
    \frac{1}{2 (1 - c)} s^2
  \end{equation}
\end{corollary}

Both results can be easily proven using the definition of the loss class and
Hoeffding's lemma.
