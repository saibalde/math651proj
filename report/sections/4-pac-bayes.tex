\section{PAC-Bayes Theory}

It can be argued that the PAC framework models learning bias through selecting
which hypothesis class to choose. This is fundamentally opposed to the Bayesian
inference setting, where the bias is modeled as choosing an appropriate prior
distribution. The PAC-Bayes theory bridges this distinction by imposing a prior
distribution on the hypothesis class. This removes the issue of ``infinite VC
dimension'' from learability of a hypothesis class.

\subsection{Learning Setup}

In the PAC-Bayes setup, we still have the training sample $S$ constructed from
i.i.d.\ sampling from distribution $\CD$ over $\CX \times \CY$, a hypothesis
class $\CH$ and a loss function $L : \CH \times \CX \times \CY \to [0, 1]$. The
new aspect, as mentioned above, is a prior distribution $\pi$ on $\CH$, and the
learning algorithm returns a posterior distribution $\hat{\rho}$. We now need to
balance the (unknown) generalization error $\Ev_{h \sim \hat{\rho}}[R_\CD^L(h)]$
given the (estimated) training error $\Ev_{h \sim \hat{\rho}}[\hat{R}_S^L(h)]$.

\subsection{A General Form of the Learning Guarantee}

The PAC-Bayesian bounds are constructed to be uniformly valid for all
distributions on the hypothesis class. These bounds are typically in terms of
the Kullback-Leibler divergence
\begin{equation}
  \KL{\rho}{\pi} = \Ev_{h \sim \rho}\left[\ln\frac{\rho(h)}{\pi(h)}\right]
\end{equation}
between the prior distribution $\pi$ (decided before seeing the sample $S$) and
any other distribution $\rho$; they are therefore valid for any posterior
distribution $\hat{\rho}$ returned by the learning algorithm. Several such
theorems exist in the literature. Here, we present a general form that can be
derived from three basic inequalities:

\begin{lemma}[Markov's Inequality]
  Let $X$ be a positive random variable. Then
  \begin{equation}
    \Pr(X \geq a) \leq \frac{\Ev[X]}{a} \iff \Pr\left(X \leq
    \frac{\Ev[X]}{\delta}\right) \geq 1 - \delta
  \end{equation}
  for all $a > 0$ and $\delta \in (0, 1)$.
\end{lemma}

\begin{lemma}[Jensen's Inequality]
  Let $X$ be a random variable and $\phi : \RR \to \RR$ a convex function. Then
  \begin{equation}
    \Ev[\phi(X)] \geq \phi(\Ev[X])
  \end{equation}
\end{lemma}

\begin{lemma}[Change of Measure Inequality]
  For any two distributions $\pi$ and $\rho$ on $\CH$ and any measurable
  function $\phi : \CH \to \RR$ we have
  \begin{equation}
    \Ev_{h \sim \rho}[\phi(h)] \leq \KL{\rho}{\pi} + \ln\left(\Ev_{h \sim
    \pi}[\exp\{\phi(h)\}]\right)
  \end{equation}
\end{lemma}

\begin{proof}
  We have
  \begin{equation}
    \begin{split}
      \Ev_{h \sim \rho}[\phi(h)]
      &= \Ev_{h \sim \rho}\left[\ln\left(\frac{\rho(h)}{\pi(h)}
      \frac{\pi(h)}{\rho(h)} \exp\{\phi(h)\}\right)\right] \\
      &= \Ev_{h \sim \rho}\left[\ln\frac{\rho(h)}{\pi(h)}\right] + \Ev_{h \sim
      \rho}\left[\ln\left(\frac{\pi(h)}{\rho(h)} \exp\{\phi(h)\}\right)\right]
      \\
      &\leq \KL{\rho}{\pi} + \ln\left(\Ev_{h \sim
      \rho}\left[\frac{\pi(h)}{\rho(h)} \exp\{\phi(h)\}\right]\right) \\
      &= \KL{\rho}{\pi} + \ln\left(\Ev_{h \sim \pi}[\exp\{\phi(h)\}]\right)
    \end{split}
  \end{equation}
  where the third step from Jensen's inequality for concave functions.
\end{proof}

\begin{theorem}[General Bound for PAC-Bayes Learning]
  \label{thm:pac-bayes}
  Given a hypothesis class $\CH$ of functions mapping from feature space $\CX$
  to label space $\CY$, for any distribution $\CD$ on $\CX \times \CY$, any loss
  function $L : \CH \times \CX \times \CY \to [0, 1]$, any prior $\pi$ over
  $\CH$, any convex function $\Delta : [0, 1] \times [0, 1] \to \RR$ and real
  number $\delta \in (0, 1)$, the following holds with probability at least $1 -
  \delta$ over samples $S \sim \CD^n$
  \begin{equation}
    \label{eq:pac-bayes-bound}
    \Delta\left(\Ev_{h \sim \rho}[R_\CD^L(h)], \Ev_{h \sim
    \rho}[\hat{R}_S^L(h)]\right) \leq \frac{1}{n} \left[\KL{\rho}{\pi} +
    \ln \frac{1}{\delta} + I_\Delta^L(\CD, n)\right]
  \end{equation}
  with
  \begin{equation}
    I_\Delta^L(\CD, n) := \ln\left(\Ev_{h \sim \pi}\left[\Ev_{S' \sim
    \CD^n}[\exp\{n \Delta(R_\CD^L(h), R_{S'}^L(h))\}]\right]\right)
  \end{equation}
  for any distribution $\rho$ on $\CH$.
\end{theorem}

\begin{proof}
  We have
  \begin{equation}
    \label{eq:pac-bayes-proof-eq-1}
    \begin{split}
      n \Delta\left(\Ev_{h \sim \rho}[R_\CD^L(h)], \Ev_{h \sim
      \rho}[\hat{R}_S^L(h)]\right)
      &\leq \Ev_{h \sim \rho}\left[n \Delta(R_\CD^L(h), \hat{R}_S^L(h))\right]
      \\
      &\leq \KL{\rho}{\pi} + \ln\left(\Ev_{h \sim \pi}[\exp\{n
      \Delta(R_\CD^L(h), \hat{R}_S^L(h))\}]\right)
    \end{split}
  \end{equation}
  where the step follows from Jensen's inequality and the second from the
  measure change inequality. Now $\Ev_{h \sim \pi}[\exp\{n \Delta(R_\CD^L(h),
  \hat{R}_S^L(h))\}]$ is a random function of $S$. Thus, from Markov's
  inequality, with probability at least $1 - \delta$ over $S \sim \CD^n$ we have
  \begin{equation}
    \begin{split}
      \Ev_{h \sim \pi}[\exp\{n \Delta(R_\CD^L(h), \hat{R}_S^L(h))\}]
      &\leq \frac{1}{\delta} \Ev_{S' \sim \CD^n}\left[\Ev_{h \sim \pi}[\exp\{n
      \Delta(R_\CD^L(h), \hat{R}_{S'}^L(h))\}]\right] \\
      &= \frac{1}{\delta} \Ev_{h \sim \pi}\left[\Ev_{S' \sim \CD^n}[\exp\{n
      \Delta(R_\CD^L(h), \hat{R}_{S'}^L(h))\}]\right]
    \end{split}
  \end{equation}
  Substituting this in \eqref{eq:pac-bayes-proof-eq-1} completes the proof.
\end{proof}

\subsection{Catoni's PAC-Bayes Bound}

We now present one particular form of the PAC-Bayes bound presented in
\cite{germain2016pac}, and attributed to \cite{catoni2007pac} therein:

\begin{theorem}
  \label{thm:catoni}
  Given a hypothesis class $\CH$ of functions mapping from feature space $\CX$
  to label space $\CY$, for any distribution $\CD$ on $\CX \times \CY$, any loss
  function $L : \CH \times \CX \times \CY \to [0, 1]$, any prior $\pi$ over
  $\CH$ and real numbers $\delta \in (0, 1)$ and $\beta > 0$ the following
  inequality holds with probability at least $1 - \delta$ over samples $S \sim
  \CD^n$
  \begin{equation}
    \label{eq:catoni-bound-unit}
    \Ev_{h \sim \rho}[R_\CD^L(h)] \leq \frac{1}{1 - e^{-\beta}} \left[1 -
    \exp\left( - \beta - \beta \Ev_{h \sim \rho}[\hat{R}_S^L(h)] - \frac{1}{n}
    \left[\KL{\rho}{\pi} + \ln \frac{1}{\delta}\right]\right)\right]
  \end{equation}
  for any distribution $\rho$ on $\CH$. \footnote{The bound cited in
  \cite{germain2016pac} and attributed to \cite{catoni2007pac} does not have the
  $-\beta$ constant term inside the exponential. I haven't been able to
  eliminate it in the proof, however.}
\end{theorem}

\begin{proof}
  We define the convex function for $\beta > 0$
  \begin{equation}
    \Delta_\beta(p, q) = -\ln[1 - (1 - e^{-\beta}) p] - \beta q
  \end{equation}
  on $[0, 1] \times [0, 1]$. It is easy to check that
  \begin{equation}
    \Delta_\beta(p, q) \leq \beta \quad \text{for all} \quad 0 \leq p, q \leq 1  
  \end{equation}
  We now compute an estimate for $I_\Delta^L(\CD, n) = \ln(\Ev_{h \sim
  \pi}[\Ev_{S' \sim \CD^n}[\exp\{n \Delta(R_\CD^L(h), R_{S'}^L(h))\}]])$:
  \begin{equation}
    \begin{split}
      &\Ev_{S' \sim \CD^n}[\exp\{n \Delta(R_\CD^L(h), \hat{R}_{S'}^L(h))\}] \\
      =& \Ev_{S' \sim \CD^n}[\exp\{- n \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)] -
      \beta n \hat{R}_{S'}^L(h)\}] \\
      =& \exp\{- n \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \Ev_{S' \sim
      \CD^n}[\exp\{-\beta n \hat{R}_{S'}^L(h))\}] \\
      =& \exp\{- n \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \Ev_{S' \sim
      \CD^n}\left[\exp\left\{-\beta \sum_{i = 1}^n L(h, x_i',
      y_i')\right\}\right] \\
      =& \exp\{- n \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \Ev_{S' \sim
      \CD^n}\left[\prod_{i = 1}^n \exp\{-\beta L(h, x_i', y_i')\}\right] \\
      =& \exp\{- n \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \prod_{i = 1}^n
      \Ev_{(x_i', y_i') \sim \CD}[\exp\{-\beta L(h, x_i', y_i')\}] \\
      =& \left[\exp\{- \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \Ev_{(x', y') \sim
      \CD}[\exp\{-\beta L(h, x', y')\}]\right]^n
    \end{split}
  \end{equation}
  where the last two steps follow from generating $S' = \{(x_1', y_1'), \ldots,
  (x_n', y_n')\}$ i.i.d.\ $\CD^n$. We further simplify the expression inside the
  brackets:
  \begin{equation}
    \begin{split}
      &\exp\{- \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\} \Ev_{(x', y') \sim
      \CD}[\exp\{-\beta L(h, x', y')\}] \\
      =& \Ev_{(x', y') \sim \CD}[\exp\{- \ln[1 - (1 - e^{-\beta}) R_\CD^L(h)]\}
      \exp\{-\beta L(h, x', y')\}] \\
      =& \Ev_{(x', y') \sim \CD}\left[\exp\{- \ln(1 - (1 - e^{-\beta}) \Ev_{(x,
      y) \sim \CD}[L(h, x, y)])\} \exp\{-\beta L(h, x', y')\}\right] \\
      \leq& \Ev_{(x', y') \sim \CD}\left[\Ev_{(x, y) \sim \CD}[\exp\{- \ln(1 -
      (1 - e^{-\beta}) L(h, x, y)) - \beta L(h, x', y')\}]\right] \\
      =& \Ev_{(x', y') \sim \CD}\left[\Ev_{(x, y) \sim \CD}[\exp\{\Delta(L(h, x,
      y)), L(h, x', y'))\}]\right]
    \end{split}
  \end{equation}
  where the inequality follows from Jensen's inequality and convexity of the
  exponential function. Now $L$ only maps to $[0, 1]$, and consequently $\Delta
  \leq \beta$ for all $(x, y), (x', y') \sim \CD$. It follows that
  \begin{equation}
      \Ev_{(x', y') \sim \CD}\left[\Ev_{(x, y) \sim \CD}[\exp\{\Delta(L(h, x,
      y)), L(h, x', y'))\}]\right] \leq e^\beta
  \end{equation}
  Combining all these inequalities, we get
  \begin{equation}
    I_\Delta^L(\CD, n) \leq \beta n
  \end{equation}
  Using this, and plugging in the expression for $\Delta$ in
  \eqref{eq:pac-bayes-bound}, we recover Catoni's bound
  \eqref{eq:catoni-bound-unit}.
\end{proof}

We can generalize this theorem to loss function mapping to any bounded interval
$[a, b]$ by defining $\beta = b - a$ and scaling the loss function $L = (b - a)
L' + a$ where $L'$ maps to $[0, 1]$. We then use the bound in
Theorem~\ref{thm:catoni} for $L'$ to obtain:
\begin{equation}
  \label{eq:catoni-bound}
  \Ev_{h \sim \rho}[R_\CD^L(h)] \leq a + \frac{b - a}{1 - e^{a - b}} \left[1 -
  \exp\left(2 a - b - \Ev_{h \sim \rho}[\hat{R}_S^L(h)] - \frac{1}{n}
  \left[\KL{\rho}{\pi} + \ln \frac{1}{\delta}\right]\right)\right]
\end{equation}
Optimizing this PAC-Bayes bound \eqref{eq:catoni-bound} over $\rho$ to find
the optimal posterior distribution would lead to a simple learning algorithm.
This optimization requires balancing the empirical expected loss $\Ev_{h \sim
\rho}[\hat{R}_S^L(h)]$ and the KL divergence between distribution $\rho$ and
prior $\pi$. This immediately leads to the following corollary:

\begin{corollary}
  \label{cor:gibbs}
  For fixed data distribution $\CD$, training sample $S$, prior distribution
  $\pi$ on hypothesis class $\CH$, bounded loss function $L$ and real number
  $\delta \in (0, 1)$, the optimal Gibbs posterior to minimize Catoni's
  PAC-Bayes bound is given by
  \begin{equation}
    \rho^*(h) = \frac{1}{Z_S} \pi(h) \exp(-n \hat{R}_S^L(h))
  \end{equation}
  where $n$ is the sample size and $Z_S$ is the normalizing constant.
\end{corollary}

\begin{proof}
  We need to minimize
  \begin{equation}
    \begin{split}
      n \Ev_{h \sim \rho}[\hat{R}_S^L(h)] + \KL{\rho}{\pi}
      &= \Ev_{h \sim \rho}\left[n \hat{R}_S^L(h) +
      \ln\frac{\rho(h)}{\pi(h)}\right] \\
      &= \Ev_{h \sim \rho}\left[\ln\frac{\rho(h)}{\rho^*(h)}\right] - \ln Z_S \\
      &= \KL{\rho}{\rho^*} - \ln Z_S
    \end{split}
  \end{equation}
  Clearly the solution is $\rho = \rho^*$.
\end{proof}
