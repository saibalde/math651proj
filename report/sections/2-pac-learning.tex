\section{A Brief Review of the PAC Learning Framework}

In this section, we briefly review the statistical aspects of supervised
learning. We primarily aim to state some of the main results from PAC learning
theory. For a full treatment of the subject, see \cite{shalev2014understanding}.

\subsection{Statistical Learning Setup}

In a supervised learning task, we are given training sample in the form of
input-output pairs $S = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ where the inputs
$x_1, \ldots, x_n$ are from some feature space $\CX$ and the outputs $y_1,
\ldots, y_n$ are from a label space $\CY$. The label space varies based on the
particular learning objective: for example, in a binary classification setup, we
will have $\CY = \{0, 1\}$, and in a regression task, we may have $\CY = \RR$.
Our goal is to learn a concept $f : \CX \to \CY$ that best fits this data
according to some loss criteria $\ell : \CY \times \CY \to \RR_+$ that measures
the distance between predicted and true labels. We also expect this learned
function $f$ to perform well on unseen features.

In a statistical setup, we assume that the data is generated i.i.d.\ from some
unknown data distribution, i.e.\ $(x_i, y_i) \sim \CD$. Our goal is to choose an
hypothesis from a hypothesis class $\CH$, consisting of functions of the form $h
: \CX \to \CY$, that best approximates the underlying concept. In this context,
we introduce a general form of the loss function $L : \CH \times \CX \times \CY
\to \RR_+$; for example, we can define
\begin{equation}
  L(h, x, y) = \ell(h(x), y)
\end{equation}
and the optimal hypothesis would reduce the expected loss of an hypothesis
\begin{equation}
  \label{eq:true-risk}
  R_\CD^L(h) := \Ev_{(x, y) \sim \CD}[L(h, x, y)]
\end{equation}

\subsection{Empirical Risk Minimization}

Of course, we cannot directly evaluate the true risk \eqref{eq:true-risk} since
we do not know the data distribution $\CD$ in advance. Instead we define the
empirical risk based on the training sample
\begin{equation}
  \label{eq:empiricial-risk}
  \hat{R}_S^L(h) := \frac{1}{n} \sum_{i = 1}^n L(h, x_i, y_i)
\end{equation}
The law of large number guarantees that as long as the samples $(x_i, y_i)$ are
sampled i.i.d.\ from the unknown data distribution, the empirical risk
approaches the true risk in the limit $n \to \infty$. This suggests we should
choose the hypothesis that minimizes this empirical risk
\begin{equation}
  \label{eq:erm}
  h^* = \Argmin_{h \in \CH} \hat{R}_S^L(h)
\end{equation}
as the optimal. This is known as the empirical risk minimization (ERM)
algorithm.

In practice, we cannot get infinite number of samples. Therefore there is always
some difference between the true risk and the empirical risk. Analyzing this
difference, in particular, how it varies with the training sample size, is the
main goal of the statistical learning theory.

\begin{remark}
  We should note that, in practice, running this ERM algorithm by evaluating the
  empirical risk for each hypothesis one by one is very often impossible due to
  computational constraints. Instead, under certain assumptions about the
  problem, this algorithm reduces to more efficient numerical algorithms, e.g.\
  gradient descent. These implementation details are, however, outside the
  context of our theoretical analysis.
\end{remark}

\begin{figure}
  \centering

  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-3.pdf}
    \caption{3rd degree fit, $L^2 \text{ error } \approx 0.46$}
  \end{subfigure}
  ~
  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-5.pdf}
    \caption{5th degree fit, $L^2 \text{ error } \approx 0.41$}
  \end{subfigure}

  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-7.pdf}
    \caption{7rd degree fit, $L^2 \text{ error } \approx 1.27$}
  \end{subfigure}
  ~
  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-9.pdf}
    \caption{9th degree fit, $L^2 \text{ error } \approx 89.71$}
  \end{subfigure}

  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-7-oversample.pdf}
    \caption{7rd degree fit, 19 samples, $L^2 \text{ error } \approx 0.41$}
  \end{subfigure}
  ~
  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/curve-fitting-9-oversample.pdf}
    \caption{9th degree fit, 36 samples, $L^2 \text{ error } \approx 0.43$}
  \end{subfigure}

  \caption{Effect of model complexity on wellness of polynomial curve fitting.
  The dashed blue curve is the true function $f(x) = \sin (2 \pi x)$. The green
  dots represent noisy observations of the function value for input $x$ drawn
  i.i.d.\ from the $\BetaDist(\frac{1}{2}, \frac{1}{2})$ distribution (10 data
  points unless otherwise indicated).  The red curve is the curve fitted to the
  data.  We control the complexity of the fitting model by specifying the degree
  of the polynomial. (\textsc{a}-\textsc{d}) As the model complexity increases,
  the generalization error, computed here as the $L^2$ distance between the true
  and fitted curves, increases after a certain threshold.
  (\textsc{e}-\textsc{f}) Increasing the number of training samples compensates
  for this increased generalization error.}
  \label{fig:curve-fitting}
\end{figure}

\subsection{Fundamental Theorems of Statistical Learning}

We can the pose the question of ``difference between empirical and true risks''
in slightly different context. The empirical risk is a measure of how our
machine learning model $h$ fits the available data. The true risk, on the other
hand also encompasses performance of our model on unseen data---it is the
generalization error. We are therefore interested in bounds on sample sizes such
that the generalization error is not too different from the training error.

From our experience in curve fitting, we know the number of training samples
needed for low-generalization error is intimately related to the complexity of
the training model. In Figure~\ref{fig:curve-fitting}, we demonstrate this
issue: as the model complexity (a.k.a.\ the degree of the polynomial being
fitted) increases, the generalization error increases after a certain threshold.
To bring this generalization error down, we need more training data.

This is a general trend that we observe in the context of supervised learning as
well: as the complexity of the hypothesis class $\CH$ is increased, we need more
training samples to pin down the generalization behavior. The probably
approximately correct (PAC) learning framework formalizes this concept:

\begin{definition}
  A hypothesis class $\CH$, consisting of functions of the form $h : \CX \to
  \CY$, is said to be PAC learnable with respect to a loss function $L : \CH
  \times \CX \times \CY \to \RR_+$ if there exists a function $n_\CH : (0, 1)
  \times (0, 1) \to \NN$ and a learning algorithm such that for every
  distribution $\CD$ on $\CX \times \CY$, when running the algorithm on training
  sample $S = \{(x_1, y_1), \ldots, (x_n, y_n)\}$, with $(x_i, y_i)$ generated
  i.i.d.\ $\CD$ and $n \geq n_\CH(\epsilon, \delta)$, the algorithm returns a
  hypothesis $h^* \in \CH$ satisfying
  \begin{equation}
    \Pr_{S \sim \CD^n}\left[R_\CD^L(h^*) \leq \min_{h \in \CH} R_\CD^L(h) +
    \epsilon\right] \geq 1 - \delta
  \end{equation}
  for all $0 < \epsilon, \delta < 1$.
\end{definition}

The fundamental theorems of statistical learning then characterize PAC
learnability of a hypothesis class in terms of their Vapnik-Chervonenkis (VC)
dimensions:

\begin{definition}
  Let $\CH$ be a class of functions of the form $h : \CX \to \{0, 1\}$, where
  $\CX$ is some arbitrary set, and $C = \{c_1, \ldots, c_n\} \subseteq \CX$.
  Then the restriction of $\CH$ to $C$ is a collection of functions from $C$ to
  $\{0, 1\}$ defined as
  \begin{equation}
    \CH\vert_C = \{(h(c_1), \ldots, h(c_n)) : h \in \CH\}
  \end{equation}
\end{definition}

\begin{definition}
  Given a class of functions $\CH$ with elements of the form $h : \CX \to \{0,
  1\}$ and a set $C = \{c_1, \ldots, c_n\}$, we say that $\CH$ shatters $C$ if
  $\Abs{\CH\vert_C} = 2^n$.
\end{definition}

\begin{definition}
  The VC dimension of a class $\CH$ of functions of the form $h : \CX \to \{0,
  1\}$ is the maximal number $n$ such that any arbitrary set $C = \{c_1, \ldots,
  c_n\} \subseteq \CX$ is shattered by $\CH$.
\end{definition}

\begin{theorem}[Fundamemntal Theorem of Statistial Learning]
  \label{thm:fundamental}
  Let $\CH$ be a hypothesis class of functions from feature space $\CX$ to label
  space $\CY = \{0, 1\}$, let $L_{01} : \CH \times \CX \times \CY \to [0, 1]$ be
  the 0-1 loss function:
  \begin{equation}
    L_{01}(h, x, y) = \mathbbm{1}_{h(x) \neq y} \quad \text{for all} \quad h \in
    \CH, x \in \CX, y \in \CY
  \end{equation}
  Then $\CH$ is PAC learnable w.r.t.\ $L_{01}$ if and only if the VC dimension
  of $\CH$ is finite. Further, if the VC dimension of $\CH$ is $d < \infty$,
  then
  \begin{equation}
    C_1 \frac{d + \log(1 / \delta)}{\epsilon} \leq n_\CH(\epsilon, \delta) \leq
    C_2 \frac{d \log(1 / \epsilon) + \log(1 / \delta)}{\epsilon}
  \end{equation}
  for constants $C_1, C_2$ and the ERM learning algorithm.
\end{theorem}

We note that while this theorem targets classification tasks with 0-1 loss
function, similar results also hold for regression tasks with absolute loss and
the squared loss. However, it does not hold for all learning tasks.

\subsection{Limitations}

Theorem~\ref{thm:fundamental} and similar theorems characterize exactly which
hypothesis classes are PAC learnable. At the same time, it is severely limited
by the ``finite VC dimension'' assumption. Many function classes have infinite
VC dimensions---a particularly simple example is given by
\begin{equation}
  h_\theta(x) = \left\lceil \sin\frac{\theta x}{2} \right\rceil, \quad x, \theta
  \in \RR
\end{equation}
The PAC theory does not provide any insight to learnability of these function
classes.
