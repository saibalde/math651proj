\section{A Brief Review of Bayesian Inference}

In this section, we briefly review the basic elements of Bayesian inference. For
a more complete treatment of the subject, see \cite{bishop2006pattern,
ghahramani2015probabilistic}.

\subsection{Primary Mechanism}

Consider a learning setup where we want to learn about parameter $\theta$ of
model $M$ from observations $S$. The Bayesian inference then has the following
general setup:
\begin{itemize}
  \item
    We choose a probability distribution $p(\theta \mid M)$ on the parameters
    from prior belief.
  \item
    We choose a likelihood $p(S \mid \theta, M)$ for our data based on the
    measurement model.
  \item
    We update our beliefs by computing the posterior distribution $p(\theta \mid
    S, M)$.
\end{itemize}
The update step is be carried out using Bayes rule:
\begin{equation}
  \label{eq:bayes-update}
  p(\theta \mid S, M) = \frac{p(\theta \mid M) p(S \mid \theta, M)}{p(S \mid M)}
  \propto p(\theta \mid M) p(S \mid \theta, M)
\end{equation}
Once we know this posterior, we can then easily estimate quantities of interest
such as the posterior mean $\Ev[\theta \mid S, M]$ or variance $\Var[\theta
\mid S, M]$.

However, unless the problem is very simple, or the prior and likelihood has
certain structures (e.g.\ conjugate priors), it is extremely difficult to
compute the posterior analytically. We therefore rely on building approximations
of the posterior. The maximum a posteriori (MAP) estimate is one of the simplest
of such approximations, where we are only interested in the mode of the
posterior distribution:
\begin{equation}
  \label{eq:map-estimate}
  \theta^\text{MAP} = \Argmax_\theta p(\theta \mid S, M) =
  \Argmax_\theta \log p(\theta \mid S, M) = \Argmax_\theta \log p(\theta \mid
  M) + \log p(S \mid \theta, M)
\end{equation}
More advanced approximation methods involve Markov chain Monte-Carlo (MCMC)
sampling or variational Bayes (VB) techniques.

\subsection{Contrast against Frequentist Approach}

In the maximum likelihood estimator (MLE) approach, we are only concerned about
the likelihood $p(S \mid \theta, M)$ of seeing the data $S$. Our goal is to find
the value of the parameter $\theta$ which maximizes this likelihood:
\begin{equation}
  \theta^\text{MLE} = \Argmax_\theta p(S \mid \theta, M) = \Argmax_\theta \log
  p(S \mid \theta, M)
\end{equation}
Comparing this with \eqref{eq:map-estimate}, we can argue that the MAP estimate
is the regularized version of the MLE. In other words, the prior distribution in
the MAP estimate allows us to put in a bias based on our prior knowledge on how
the parameter $\theta$ should behave. For example, if we want to enforce that
$\theta \in \RR^d$ should be a sparse vector, then we would want a regularizer
of the form
\begin{equation}
  \log p(\theta \mid M) = - \lambda \sum_{i = 1}^d \Abs{\theta_i} +
  \text{const.} \implies p(\theta \mid M) \propto \exp\left(-\lambda \sum_{i =
  1}^d \Abs{\theta_i}\right)
\end{equation}
i.e.\ we should impose a Laplace prior on the parameters.

% \subsection{Bayesian Model Selection}
%
% The Bayesian inference framework also provides a systematic model selection
% procedure. Let us assume that we have $k$ models $M_1, \ldots, M_k$. Then
% using Bayes rule, we get
% \begin{equation}
%   p(M_i \mid S) = \frac{p(M_i) p(S \mid M_i)}{p(S)} \quad \text{where} \quad
%   p(S \mid M_i) = \int p(\theta, S \mid M_i) d\theta = \int p(\theta \mid M_i)
%   p(S \mid \theta, M_i) d\theta
% \end{equation}
% It follows that
% \begin{equation}
%   \frac{p(M_i \mid S)}{p(M_j \mid S)} = \frac{p(M_i) p(S \mid M_i)}{p(M_j) p(S
%   \mid M_j)}
% \end{equation}
% We usually put uniform priors on the models themselves. Then
% \begin{equation}
%   \frac{p(M_i \mid S)}{p(M_j \mid S)} = \frac{p(S \mid M_i)}{p(S \mid M_j)}
% \end{equation}
% The quantity on the right is called the Bayes factor between models $M_i$ and
% $M_j$, and we select $M_i$ over $M_j$ if is larger than $1$.
